from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import GPT4All
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import gradio as gr

# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
loader = DirectoryLoader("./documents", glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()

# ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
docs = splitter.split_documents(documents)

# Embeddingä½œæˆ
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectordb = FAISS.from_documents(docs, embedding)
vectordb.save_local("./embeddings")

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå®šç¾©
custom_prompt = PromptTemplate.from_template("""
ã‚ãªãŸã¯ç¤¾å†…è¦ç´„ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚è€ƒã«ã€è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã‹ã¤ä¸å¯§ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
{context}

# è³ªå•:
{question}

# å›ç­”ï¼ˆæ•¬èªã§ã€ã‚ã‹ã‚Šã‚„ã™ãï¼‰:
""")

# LLMãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
llm = GPT4All(
    model="./models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    backend="llama",
    max_tokens=256,
    verbose=True
)

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®Retrieverï¼ˆæ–‡æ›¸æ•°åˆ¶é™ï¼‰
retriever = vectordb.as_retriever(search_kwargs={"k": 1})

# QAãƒã‚§ãƒ¼ãƒ³
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": custom_prompt}
)

# ãƒãƒ£ãƒƒãƒˆUI
def chat_fn(message):
    return qa.run(message)

gr.Interface(fn=chat_fn, inputs="text", outputs="text", title="ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆğŸ’¼").launch()
